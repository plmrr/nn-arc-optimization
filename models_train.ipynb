{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Wczytanie gotowego zestawu przetworzonych danych"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/preprocessed_data.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wczytanie predykcji PSE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pse = pd.read_csv('data/predictions_pse.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Podzial danych na zbiór treningowy, walidacjny i testowy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = df.copy()\n",
    "\n",
    "test_24h_data = data.iloc[-48:-24]\n",
    "data = data.iloc[:-48]\n",
    "\n",
    "train_data, temp_data = train_test_split(data, test_size=0.1, shuffle=False)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Normalizacja danych"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "scalers = {}\n",
    "train_scaled = train_data.copy()\n",
    "for column in train_data.columns:\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled[column] = scaler.fit_transform(train_data[column].values.reshape(-1, 1))\n",
    "    scalers[column] = scaler\n",
    "    joblib.dump(scaler, f'scalers/scaler_{column}.pkl')\n",
    "\n",
    "def scale_data(data, scalers):\n",
    "    data_scaled = data.copy()\n",
    "    for column in data.columns:\n",
    "        scaler = scalers[column]\n",
    "        data_scaled[column] = scaler.transform(data[column].values.reshape(-1, 1))\n",
    "    return data_scaled\n",
    "\n",
    "\n",
    "val_scaled = scale_data(val_data, scalers)\n",
    "test_scaled = scale_data(test_data, scalers)\n",
    "test_24h_scaled = scale_data(test_24h_data, scalers)\n",
    "test_24h_scaled = np.array([test_24h_scaled])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utworzenie sekwencji danych"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_sequences(dataset, look_back, forecast_horizon):\n",
    "    data_values = dataset.values\n",
    "    num_elements = data_values.shape[0]\n",
    "    X = [data_values[i:i+look_back] for i in range(num_elements - look_back - forecast_horizon + 1)]\n",
    "    Y = [data_values[i+look_back:i+look_back+forecast_horizon, 0] for i in range(num_elements - look_back - forecast_horizon + 1)]\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "look_back = 24\n",
    "forecast_horizon = 24\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(train_scaled, look_back, forecast_horizon)\n",
    "X_val_seq, y_val_seq = create_sequences(val_scaled, look_back, forecast_horizon)\n",
    "X_test_seq, y_test_seq = create_sequences(test_scaled, look_back, forecast_horizon)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Definicja funkcji błędów"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metryki błedów oraz wizualizacje predykcji dla poszczególnych modeli"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plots_and_metrics(model):\n",
    "    train_predict = model.predict(X_train_seq)\n",
    "    val_predict = model.predict(X_val_seq)\n",
    "    test_predict = model.predict(X_test_seq)\n",
    "\n",
    "    train_predict_original = scalers['electricity_load'].inverse_transform(train_predict)\n",
    "    y_train_original = scalers['electricity_load'].inverse_transform(y_train_seq)\n",
    "    val_predict_original = scalers['electricity_load'].inverse_transform(val_predict)\n",
    "    y_val_original = scalers['electricity_load'].inverse_transform(y_val_seq)\n",
    "    test_predict_original = scalers['electricity_load'].inverse_transform(test_predict)\n",
    "    y_test_original = scalers['electricity_load'].inverse_transform(y_test_seq)\n",
    "\n",
    "    print('Train RMSE:', np.sqrt(mean_squared_error(y_train_original, train_predict_original)))\n",
    "    print('Validation RMSE:', np.sqrt(mean_squared_error(y_val_original, val_predict_original)))\n",
    "    print('Test RMSE:', np.sqrt(mean_squared_error(y_test_original, test_predict_original)))\n",
    "    print('Train MAPE:', mape(y_train_original, train_predict_original))\n",
    "    print('Validation MAPE:', mape(y_val_original, val_predict_original))\n",
    "    print('Test MAPE:', mape(y_test_original, test_predict_original))\n",
    "\n",
    "    predictions_24h = model.predict(test_24h_scaled)\n",
    "    predictions_original = scalers['electricity_load'].inverse_transform(predictions_24h)\n",
    "    predictions_original = predictions_original.reshape(-1).tolist()\n",
    "    actual_values = df_pse['electricity_load'].tail(24).values\n",
    "    pse_predictions = df_pse['forecasted_load'].tail(24).values\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(actual_values, label='Rzeczywiste zapotrzebowanie', color='green', linewidth=1.8)\n",
    "    plt.plot(pse_predictions, label='Predykcja PSE', color='red', linewidth=0.8)\n",
    "    plt.plot(predictions_original, label='Predykcja modelu', color='blue', linewidth=0.8)\n",
    "    plt.xlabel('Godzina')\n",
    "    plt.ylabel('Zapotrzebowanie [MW]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"MAPE predykcji opracowanego modelu:\", mape(actual_values, predictions_original))\n",
    "    print(\"MAPE predykcji PSE:\", mape(actual_values, pse_predictions))\n",
    "    print('RMSE predykcji opracowanego modelu:', np.sqrt(mean_squared_error(actual_values, predictions_original)))\n",
    "    print('RMSE predykcji PSE: ', np.sqrt(mean_squared_error(actual_values, pse_predictions)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wizualizacja funkcji strat w procesie uczenia dla poszczególnych modeli"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_history(hist):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.ylabel('Strata')\n",
    "    plt.xlabel('Epoka')\n",
    "    plt.legend(['Zestaw treningowy', 'Zestaw walidacyjny'], loc='upper right')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---------------------------------------\n",
    "# Utworzenie oraz uczenie sieci FFANN\n",
    "---------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(24, 7)))\n",
    "model.add(Dense(168, activation='relu'))\n",
    "model.add(Dense(126, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(24, activation='linear'))\n",
    "model.compile(optimizer='adam', loss=rmse)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "mc = ModelCheckpoint('model_1_ffann.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_seq, y_train_seq, epochs=100, batch_size=64, validation_data=(X_val_seq, y_val_seq), callbacks=[es, mc], shuffle=False)\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "plots_and_metrics(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(24, 7)))\n",
    "model.add(Dense(168, activation='relu'))\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dense(72, activation='relu'))\n",
    "model.add(Dense(48, activation='relu'))\n",
    "model.add(Dense(24, activation='linear'))\n",
    "model.compile(optimizer='adam', loss=rmse)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "mc = ModelCheckpoint('model_2_ffann.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_seq, y_train_seq, epochs=100, batch_size=64, validation_data=(X_val_seq, y_val_seq), callbacks=[es, mc], shuffle=False)\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "plots_and_metrics(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(24, 7)))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(24, activation='linear'))\n",
    "model.compile(optimizer='adam', loss=rmse)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "mc = ModelCheckpoint('model_3_ffann', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_seq, y_train_seq, epochs=100, batch_size=64, validation_data=(X_val_seq, y_val_seq), callbacks=[es, mc], shuffle=False)\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "plots_and_metrics(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---------------------------------------\n",
    "# Utworzenie oraz uczenie sieci FFANN wraz z optymalizacją PSO\n",
    "---------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 4, 5, 6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "NEURONS_RANGE = (25, 200)\n",
    "\n",
    "\n",
    "def create_model(neurons_per_layer):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(24, 7)))\n",
    "    for neurons in neurons_per_layer:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(24, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss=rmse)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def random_hyperparameters(n_layers):\n",
    "    hyperparameters = []\n",
    "    for _ in range(n_layers - 1):\n",
    "        neurons = random.randint(*NEURONS_RANGE)\n",
    "        hyperparameters.append(neurons)\n",
    "    hyperparameters.append(random.randint(*NEURONS_RANGE))\n",
    "    return hyperparameters\n",
    "\n",
    "\n",
    "def random_velocity():\n",
    "    v_neurons = random.uniform(-1, 1)\n",
    "    return v_neurons\n",
    "\n",
    "\n",
    "def update_velocity(particle, global_best_position, w=0.5, c1=1, c2=2):\n",
    "    new_velocity = []\n",
    "    for i, v_neurons in enumerate(particle.velocity):\n",
    "        r1 = random.random()\n",
    "        r2 = random.random()\n",
    "        best_neurons = particle.best_position[i]\n",
    "        current_neurons = particle.position[i]\n",
    "        if global_best_position is not None and i < len(global_best_position):\n",
    "            global_best_neurons = global_best_position[i]\n",
    "        else:\n",
    "            global_best_neurons = current_neurons\n",
    "        cognitive_velocity_neurons = c1 * r1 * (best_neurons - current_neurons)\n",
    "        social_velocity_neurons = c2 * r2 * (global_best_neurons - current_neurons)\n",
    "        v_neurons_updated = w * v_neurons + cognitive_velocity_neurons + social_velocity_neurons\n",
    "        new_velocity.append(v_neurons_updated)\n",
    "    particle.velocity = new_velocity\n",
    "\n",
    "\n",
    "def update_position(particle):\n",
    "    new_position = []\n",
    "    for i, v_neurons in enumerate(particle.velocity):\n",
    "        current_neurons = particle.position[i]\n",
    "        new_neurons = max(min(current_neurons + v_neurons, NEURONS_RANGE[1]), NEURONS_RANGE[0])\n",
    "        new_position.append(new_neurons)\n",
    "    particle.position = new_position\n",
    "\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self, n_layers):\n",
    "        self.position = random_hyperparameters(n_layers)\n",
    "        self.velocity = [random_velocity() for _ in range(n_layers)]\n",
    "        self.best_position = self.position\n",
    "        self.best_error = float('inf')\n",
    "\n",
    "\n",
    "def pso(n_particles, n_iterations, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    global_best_position = None\n",
    "    global_best_error = float('inf')\n",
    "    no_improve_count = 0\n",
    "    particles = [Particle(random.randint(1, 5)) for _ in range(n_particles)]\n",
    "    for iteration in range(n_iterations):\n",
    "        improved = False\n",
    "        for particle in particles:\n",
    "            model = create_model(particle.position)\n",
    "            es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7, restore_best_weights=True)\n",
    "            history_best = model.fit(X_train, y_train, epochs=1, batch_size=64, validation_data=(X_val, y_val), callbacks=[es], shuffle=False)\n",
    "            predictions = model.predict(X_test)\n",
    "            error = rmse(y_test, predictions)\n",
    "            if error < particle.best_error:\n",
    "                particle.best_error = error\n",
    "                particle.best_position = particle.position\n",
    "            if error < global_best_error:\n",
    "                global_best_error = error\n",
    "                global_best_position = particle.best_position.copy()\n",
    "                model.save('pso_best_model.h5')\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.plot(history_best.history['loss'])\n",
    "                plt.plot(history_best.history['val_loss'])\n",
    "                plt.ylabel('Strata')\n",
    "                plt.xlabel('Epoka')\n",
    "                plt.legend(['Zestaw treningowy', 'Zestaw walidacyjny'], loc='upper right')\n",
    "                plt.savefig('pso_best_model_history.png')\n",
    "                plt.close()\n",
    "                improved = True\n",
    "                no_improve_count = 0\n",
    "\n",
    "        if not improved:\n",
    "            no_improve_count += 1\n",
    "\n",
    "        if no_improve_count >= 5:\n",
    "            print(f\"Brak poprawy globalnej straty przez {iteration+1}. Zatrzymywanie działania algorytmu PSO...\")\n",
    "            return global_best_position\n",
    "\n",
    "        for particle in particles:\n",
    "            update_velocity(particle, global_best_position)\n",
    "            update_position(particle)\n",
    "        print(f\"Iteration {iteration+1}, Best Error: {global_best_error}\")\n",
    "    return global_best_position\n",
    "\n",
    "\n",
    "hyperparameters = pso(n_particles=15, n_iterations=40, X_train=X_train_seq, y_train=y_train_seq, X_val=X_val_seq, y_val=y_val_seq, X_test=X_test_seq,y_test=y_test_seq)\n",
    "print(hyperparameters)\n",
    "\n",
    "model = tf.keras.models.load_model('pso_best_model.h5', custom_objects={'rmse': rmse})\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "plots_and_metrics(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---------------------------------------\n",
    "# Utworzenie oraz uczenie sieci LSTM\n",
    "---------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 7"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(24))\n",
    "model.compile(optimizer='adam', loss=rmse)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7, restore_best_weights=True)\n",
    "mc = ModelCheckpoint('model_7_lstm.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_seq, y_train_seq, epochs=1, batch_size=16, validation_data=(X_val_seq, y_val_seq), callbacks=[es, mc], shuffle=False)\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "plots_and_metrics(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(24))\n",
    "model.compile(optimizer='adam', loss=rmse)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7, restore_best_weights=True)\n",
    "mc = ModelCheckpoint('model_8_lstm.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_seq, y_train_seq, epochs=100, batch_size=16, validation_data=(X_val_seq, y_val_seq), callbacks=[es, mc], shuffle=False)\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "plots_and_metrics(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 9"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(168, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(24))\n",
    "model.compile(optimizer='adam', loss=rmse)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7, restore_best_weights=True)\n",
    "mc = ModelCheckpoint('model_9_lstm.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_seq, y_train_seq, epochs=100, batch_size=16, validation_data=(X_val_seq, y_val_seq), callbacks=[es, mc], shuffle=False)\n",
    "plot_history(history)\n",
    "\n",
    "plots_and_metrics(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
